from bs4 import BeautifulSoup  # HTML íŒŒì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.request import Request, urlopen  # ì›¹ ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import quote  # URL ì¸ì½”ë”©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import datetime  # ë‚ ì§œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import csv  # CSV íŒŒì¼ ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ====================================================================================================
# ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_total_pages(url):
    urlrequest = Request(url, headers={'User-Agent': 'Mozilla/5.0'})  # ì›¹ ìš”ì²­ ìƒì„± (User-Agent ì„¤ì •ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€)
    html = urlopen(urlrequest)  # URL ì—´ê¸°
    soup = BeautifulSoup(html, 'html.parser')  # HTML íŒŒì‹±

    # í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ ì°¾ê¸°
    pagination = soup.find('div', {'class': 'pagination'})
    total_pages = 1  # ê¸°ë³¸ í˜ì´ì§€ ìˆ˜ëŠ” 1 (í˜ì´ì§€ë„¤ì´ì…˜ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„)
    if pagination:
        page_numbers = pagination.find_all('a', {'class': 'page'})  # í˜ì´ì§€ ë²ˆí˜¸ê°€ ë“¤ì–´ ìˆëŠ” ë§í¬ ê²€ìƒ‰
        if page_numbers:
            total_pages = int(page_numbers[-1].text)  # ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ê°€ ì „ì²´ í˜ì´ì§€ ìˆ˜

    return total_pages

# ì±„ìš© ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def get_sector_data(url, keyword, sector_list, max_pages):
    try:
        for page in range(1, max_pages + 1):  # 1í˜ì´ì§€ë¶€í„° max_pagesê¹Œì§€ ìˆœíšŒ
            paginated_url = url + f'&recruitPage={page}&recruitSort=relation&recruitPageCount=1000'  # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
            urlrequest = Request(paginated_url, headers={'User-Agent': 'Mozilla/5.0'})  # ì›¹ ìš”ì²­ ìƒì„±
            html = urlopen(urlrequest)  # URL ì—´ê¸°
            soup = BeautifulSoup(html, 'html.parser')  # HTML íŒŒì‹±

            # ì±„ìš© ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            job_items = soup.find_all('div', {'class': 'item_recruit'})

            # ê° ì±„ìš© ê³µê³ ì—ì„œ ì •ë³´ ì¶”ì¶œ
            for job in job_items:
                area_job = job.find('div', {'class': 'area_job'})  # ì§ë¬´ ì •ë³´ê°€ ìˆëŠ” ì˜ì—­
                company_tag = job.find('div', {'class': 'area_corp'})  # íšŒì‚¬ ì •ë³´ê°€ ìˆëŠ” ì˜ì—­
                
                # ì±„ìš© ê³µê³  ì œëª© ë° ë§í¬ ê°€ì ¸ì˜¤ê¸°
                job_title_tag = area_job.find('h2', {'class': 'job_tit'}).find('a')
                
                # ì§ë¬´ ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
                job_condition = area_job.find('div', {'class': 'job_condition'})
                job_info = job_condition.find_all('span')  # ìœ„ì¹˜, ê²½ë ¥, í•™ë ¥, ê¸‰ì—¬ ë“±ì˜ ì •ë³´ í¬í•¨
                
                # ê²€ìƒ‰ì–´ì™€ ê´€ë ¨ëœ ì±„ìš© ê³µê³ ì¸ì§€ í™•ì¸
                if job_title_tag and 'title' in job_title_tag.attrs:
                    job_title = job_title_tag['title'].strip()  # ì±„ìš© ê³µê³  ì œëª©
                    job_link = 'https://www.saramin.co.kr' + str(job_title_tag.attrs['href'])  # ì±„ìš© ê³µê³  ë§í¬
                    
                    company_name = company_tag.find('strong', {'class': 'corp_name'}).text.strip()  # íšŒì‚¬ëª…
                    
                    # ì§ë¬´ ì¡°ê±´ ì •ë³´ ì¶”ì¶œ (ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ N/A)
                    location = job_info[0].text.strip() if len(job_info) > 0 else 'N/A'
                    career = job_info[1].text.strip() if len(job_info) > 1 else 'N/A'
                    education = job_info[2].text.strip() if len(job_info) > 2 else 'N/A'
                    salary = job_info[3].text.strip() if len(job_info) > 3 else 'N/A'
                    
                    # ì±„ìš© ê³µê³  ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
                    job_details = {
                        'íšŒì‚¬ëª…': company_name,
                        'ì±„ìš©ê³µê³ ì‚¬í•­': job_title,
                        'ë§í¬': job_link,
                        'ì§€ì—­': location,
                        'ê²½ë ¥': career,
                        'í•™ë ¥': education,
                        'ì›”ê¸‰': salary
                    }
                    
                    # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    sector_list.append(job_details)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# CSV íŒŒì¼ ë§Œë“¤ê¸°
def make_CSV(search_word):
    if not search_word:
        print("âŒ ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return None
    
    # ê²€ìƒ‰ì–´ URL ì¸ì½”ë”© (í•œê¸€ ê¹¨ì§ ë°©ì§€)
    encoded_search_word = quote(search_word)

    # ê²€ìƒ‰ URL ìƒì„±
    url = f'https://www.saramin.co.kr/zf_user/search?' \
          f'search_area=main&search_done=y&search_optional_item=n&searchType=search&' \
          f'searchword={encoded_search_word}'

    # ì±„ìš© ì •ë³´ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    sector_list = []

    # ì˜¤ëŠ˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    today = datetime.datetime.now().strftime('%y-%m-%d')

    # ì´ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    total_pages = get_total_pages(url)
    print(f'ğŸ“Œ ì´ í˜ì´ì§€ ìˆ˜: {total_pages}, ë‚ ì§œ: {today}, ê²€ìƒ‰ì–´: {search_word}')

    # ì±„ìš© ì •ë³´ ìˆ˜ì§‘
    get_sector_data(url, search_word, sector_list, total_pages)

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    job_saramin_df = pd.DataFrame(sector_list)

    # íŒŒì¼ëª… ì„¤ì • (ê²€ìƒ‰ì–´ ê¸°ë°˜)
    file = f'{search_word}_ì±„ìš©ê³µê³ .csv' 

    # CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)
    job_saramin_df.to_csv(file, encoding="utf-8-sig", index=False)

    # ê²°ê³¼ ì¶œë ¥
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {file}")
    print(job_saramin_df)
    return file

# âœ… ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°ì—ë§Œ ë™ì‘ (import ì‹œ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
if __name__ == "__main__":
    search_word = input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    make_CSV(search_word)

